# AUTOGENERATED! DO NOT EDIT! File to edit: 02_analysis_star.ipynb (unless otherwise specified).

__all__ = ['say_hello', 'nlp_en', 'rep', 'fn_remove_punc', 'clean_punc', 'remove_stopword', 'nlp', 'cal_lemma',
           'set_key', 'cal_lemma_pos', 'return_wc', 'plot_wc', 'set_key', 'separate_pos', 'calculate_ngram', 'nlp',
           'stoplist', 'visulaizeBigrams', 'plot_multi_wc', 'plot_bigram', 'visulaizeBigrams_multi']

# Cell
def say_hello(to):
    "Say hello to somebody"
    return f'Hello {to}!'

# Cell
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import os
pd.set_option('display.max_columns', 500)
pd.set_option('display.max_colwidth', -1)
from nltk import sent_tokenize
import nltk
# import gensim
import string
from spacy.lang.en import English
from spacy.lang.en.stop_words import STOP_WORDS
from tqdm import tqdm
import spacy
import re
nlp_en = spacy.load("en_core_web_sm")

# Cell
def rep(x):
    return re.sub('\s+',' ',x)

fn_remove_punc = lambda x: x.lower().replace('..','.').replace('/',' ').replace('"','')
# fn_rep = lambda x: rep(x)
# .replace(',','.').replace(';','.').replace(' he ','teacher ').replace(' she ','teacher ')

# Cell
def clean_punc(string,translator):
    string = string.translate(translator)

    return string

## remove stopwords
nlp = spacy.load("en_core_web_sm")

def remove_stopword(string,nlp):
    doc = nlp(string)

    token_list = []
    for token in doc:
        token_list.append(token.text)

    filtered_sentence =[]
    for word in token_list:
        lexeme = nlp.vocab[word]
        if lexeme.is_stop == False:
            filtered_sentence.append(word)

    return ' '.join(i for i in filtered_sentence)

# Cell
def cal_lemma(string):
    doc = nlp(string)
    y = [x.lemma_ for x in doc]
    return ' '.join(i for i in y)

# Cell
# calculate POS and Lemma

def set_key(dictionary, key, value):
    if key not in dictionary:
        dictionary[key] = value
    elif type(dictionary[key]) == list:
        if value not in dictionary[key]:
            dictionary[key].append(value)
    else:
        dictionary[key] = [dictionary[key], value]

def cal_lemma_pos(x):
    s={}
    try:
        doc = nlp(x)
        y=[(x.pos_,x.lemma_) for x in doc]

        for i,j in y:
            set_key(s,i,j)
    except:
        pass
    return s

# Cell
def return_wc(x,max_word=50,facecolor='k',bg_color='white'):
    wordcloud = WordCloud(background_color=bg_color,width=1600,
                          stopwords=nlp.Defaults.stop_words,max_words=max_word,
                          height=800).generate(x)
    return wordcloud

def plot_wc(x,max_word=50,facecolor='k',title='None',bg_color='white'):
    wordcloud = return_wc(x,max_word,facecolor,bg_color)
    plt.figure( figsize=(20,10),facecolor=facecolor)
    if title !='None':
        plt.title(title,fontsize=50)
        plt.axis('off')
    plt.imshow(wordcloud)

# Cell
def set_key(dictionary, key, value):
    if key not in dictionary:
        dictionary[key] = value
    elif type(dictionary[key]) == list:
        dictionary[key].append(value)
    else:
        dictionary[key] = [dictionary[key], value]

def separate_pos(df):
#     df['co_star_pos_lemma_dict'] = df['co_star_pos_lemma'].progress_apply(lambda x: yaml.load(x))
    df = df.reset_index()
    lemma_dict = df['co_star_pos_lemma_dict'].to_dict()
    lemma_dict_c = lemma_dict.copy()

    comp_dict = {}
#     print(len(lemma_dict_c))
    for lm in range(len(lemma_dict_c)):
        dict1 = lemma_dict_c[lm]
        keys = dict1.keys()
        for key in keys:
            set_key(comp_dict,key,dict1[key])

    verbs = ' '.join(list(flatten(comp_dict['VERB'])))
    noun = ' '.join(list(flatten(comp_dict['NOUN'])))
    adj = ' '.join(list(flatten(comp_dict['ADJ'])))
    adv = ' '.join(list(flatten(comp_dict['ADV'])))

    return verbs,noun,adj,adv

# Cell
nlp = spacy.load("en_core_web_sm")
stoplist = nlp.Defaults.stop_words

def calculate_ngram(df,col='co_star_clean'):
    c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))
    ngrams = c_vec.fit_transform(df[col])
    ngrams = ngrams.astype(np.int8)
    count_values = ngrams.toarray().sum(axis=0)

    vocab = c_vec.vocabulary_
    df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)
                ).rename(columns={0: 'frequency', 1:'bigram'})
    return df_ngram[:50]


# Cell
def visulaizeBigrams(bigram_df, K):
    d = bigram_df.set_index('bigram_tuple').T.to_dict('records')
#     print(d)
    # Create network plot
    G = nx.Graph()

    # Create connections between nodes
    for k, v in d[0].items():
#         print(k[0],v)
        G.add_edge(k[0], k[1], weight=(v * 10))

    #G.add_node("china", weight=100)
    fig, ax = plt.subplots(figsize=(20, 16))

    # k : float (default=None)
    # Optimal distance between nodes.
    # If None the distance is set to 1/sqrt(n) where n is the number of nodes. Increase this value to move nodes farther apart.
    pos = nx.spring_layout(G, k=K)

    # Plot networks
    nx.draw_networkx(G, pos,
                     font_size=13,
                     width=2,
                     edge_color='grey',
                     node_color='purple',
                     with_labels = False,
                     ax=ax)

    # Create offset labels
    for key, value in pos.items():
        x, y = value[0]+.02, value[1]+.045
#         print(key)
        ax.text(x, y,
                s=key,
                bbox=dict(facecolor='red', alpha=0.25),
                horizontalalignment='center', fontsize=13)
    plt.grid(True)
    plt.show()

# Cell
def plot_multi_wc(n_topic,word_clouds,texts):
    nrows=n_topic//2
    fig, axs = plt.subplots(nrows,2, figsize=(16, 8*n_topic//4), facecolor='w', edgecolor='k')
    fig.subplots_adjust(hspace = .2, wspace=.05)
    axs = axs.ravel()

    for i,[text, wc] in enumerate(zip(texts,word_clouds)):
        axs[i].imshow(wc)
        axs[i].axis('off')
        axs[i].set_title(text,fontsize=20)

# Cell
def plot_bigram(bigrams,texts):
    nrows = len(bigrams)//2
    fig, axs = plt.subplots(nrows,2, figsize=(20,15*nrows))
    fig.subplots_adjust(hspace = .2, wspace=.5)
    axs = axs.ravel()

    for i,[text,df] in enumerate(zip(texts,bigrams)):
        sns.barplot(df.frequency,df['bigram'],ax=axs[i],palette='tab20b')
        axs[i].set_title(text,fontsize=20)

# Cell
def visulaizeBigrams_multi(bigram_dfs, texts,K):
    fig, axs = plt.subplots(2,2, figsize=(30,12*2))
    fig.subplots_adjust(hspace = 0.1, wspace=.05)
    axs = axs.ravel()

    for i, [bigram_df,text] in enumerate(zip(bigram_dfs,texts)):
        d = bigram_df.set_index('bigram_tuple').T.to_dict('records')
    #     print(d)
        # Create network plot
        G = nx.Graph()

        # Create connections between nodes
        for k, v in d[0].items():
    #         print(k[0],v)
            G.add_edge(k[0], k[1], weight=(v * 10))
        pos = nx.spring_layout(G, k=K)

        # Plot networks
        nx.draw_networkx(G, pos,
                         font_size=13,
                         width=2,
                         edge_color='grey',
                         node_color='purple',
                         with_labels = False,
                         ax=axs[i])

        # Create offset labels
        for key, value in pos.items():
            x, y = value[0]+.02, value[1]+.045
    #         print(key)
            axs[i].text(x, y,
                    s=key,
                    bbox=dict(facecolor='red', alpha=0.25),
                    horizontalalignment='center', fontsize=15)
            axs[i].grid(True)
            axs[i].set_title(text,fontsize=30)
